{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c225caf7-a12e-47f9-a3f7-40692e30aa37",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial P_i}{\\partial t} = \\mu_{ref} \\cdot T_f \\cdot \\left(1- exp\\left(\\frac{-\\alpha_i^{Chl}\\ \\theta_i^C\\ I}{\\mu_{ref}\\ T_f\\ V_i}\\right)\\right) \\cdot V_i \\cdot P_i - g_z^{\\max} \\cdot Z \\cdot \\frac{P_i}{K_{P_i} + P_i} - m_i \\cdot T_f \\cdot P_i - \\alpha_i \\cdot P_i^{1.75}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411e538-d463-4e53-be1f-98c35a4c60ce",
   "metadata": {},
   "source": [
    "$\\mu_{ref} = 5.0 d^{-1}$                                   Maximum C-spec growth rate at $T_{ref}$ for small phytoplankton and diatoms  \n",
    "$T_{ref} = 30 ^\\circ C$                                    Refrence temperature  \n",
    "$\\theta_{max,sp}^N = 2.5\\ mg Chl/mmol$                     Maximum $\\theta^N (Chl/N)$ for small phytoplankton  \n",
    "$\\theta_{max,diat}^N = 4.0\\ mg Chl/mmol$                   Maximum $\\theta^N (Chl/N)$ for diatoms  \n",
    "$\\alpha_{sp}^{Chl} = 0.39\\ mmol\\ m^2/(mg\\ Chl\\ W\\ day)$    Chl-spec initial slope of P_I curve for small phytoplankton\n",
    "$\\alpha_{diat}^{Chl} = 0.28\\ mmol\\ m^2/(mg\\ Chl\\ W\\ day)$  Chl-spec initial slope of P_I curve for diatoms  \n",
    "$K_{sp}^{Dfe} = 3.0e-05\\ mmol/m^3$                         Fe uptake half-sat constant for small phytoplankton  \n",
    "$K_{diat}^{Dfe} = 7.0e-05\\ mmol/m^3$                       Fe uptake half-sat constant for diatoms  \n",
    "$K_{sp}^{NO^3} = 0.25\\ mmol/m^3$                           $NO^3$ uptake half-sat constant for small phytoplankton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72e6fa-6a84-4b97-bc03-5c19cc9da405",
   "metadata": {},
   "source": [
    "The maximum C-spec growth rate at \\( T_{ref} \\) for small phytoplankton and diatoms is given by:\n",
    "\n",
    "$\\mu_{ref} = 5.0\\; d^{-1}$  \n",
    "\n",
    "The reference temperature is given by \\( T_{ref} = 30^\\circ\\; \\mathrm{C} \\).\n",
    "\n",
    "For small phytoplankton:\n",
    "\n",
    "$\\theta_{max,sp}^N = 2.5\\; mg\\; Chl/mmol$  \n",
    "\n",
    "For diatoms:\n",
    "\n",
    "$\\theta_{max,diat}^N = 4.0\\; mg\\; Chl/mmol$  \n",
    "\n",
    "The Chl-spec initial slope of the P-I curve for small phytoplankton is:\n",
    "\n",
    "$\\alpha_{sp}^{Chl} = 0.39\\; \\mathrm{mmol}\\; m^{-2}\\; (mg\\; Chl\\; W\\; day)^{-1}$  \n",
    "\n",
    "And for diatoms:\n",
    "\n",
    "$\\alpha_{diat}^{Chl} = 0.28\\; \\mathrm{mmol}\\; m^{-2}\\; (mg\\; Chl\\; W\\; day)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a094e8-7c82-4586-a595-1b41192c8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fb8372-c1a7-47fe-a1cb-e8be0a43d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuromancer imports (symbolic API)\n",
    "import neuromancer as nm\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.system import Node, System\n",
    "from neuromancer.dynamics import integrators, ode\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.constraint import variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a90ef1-37f2-4b1e-8334-1bca9b71ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"regridded_data/2015-2021_combined.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9f4fa7-65f3-4083-8865-ef084cec7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              dfe       no3       po4       sil       rsn  \\\n",
      "lon    lat   month_year                                                     \n",
      "-179.5 -76.5 2015-01-01  0.042463  2.457866  5.414602  2.663714  6.223525   \n",
      "             2015-02-01  0.056386  2.517316  5.486631  2.416398  3.783490   \n",
      "             2015-12-01  0.048093  2.668628  5.709538  3.263838  8.403666   \n",
      "             2016-01-01  0.033411  2.665089  5.803986  3.021275  5.299753   \n",
      "             2016-02-01  0.066006  2.360248  5.209874  2.289098  3.853204   \n",
      "\n",
      "                              mld       ssh       sal       sst      chla  \\\n",
      "lon    lat   month_year                                                     \n",
      "-179.5 -76.5 2015-01-01  0.031551  1.773168  7.995484  0.429358  0.053512   \n",
      "             2015-02-01  0.096969  1.718363  8.200290  0.225703  0.076863   \n",
      "             2015-12-01  0.076738  1.647681  8.305805  0.355078  0.401371   \n",
      "             2016-01-01  0.089604  1.587883  8.291054  0.466634  0.112831   \n",
      "             2016-02-01  0.050680  1.588911  8.109305  0.260187  0.433956   \n",
      "\n",
      "                         ...      nano      pico   logmcro   lognano  \\\n",
      "lon    lat   month_year  ...                                           \n",
      "-179.5 -76.5 2015-01-01  ...  0.032269  1.710984  4.602053  5.598274   \n",
      "             2015-02-01  ...  0.042806  2.291855  5.135406  5.815009   \n",
      "             2015-12-01  ...  0.080049  5.659362  7.273804  6.295207   \n",
      "             2016-01-01  ...  0.054900  3.089935  5.683156  6.005877   \n",
      "             2016-02-01  ...  0.081099  5.576194  7.358690  6.305210   \n",
      "\n",
      "                          logpico     V_dfe     V_no3     V_po4        Tf  \\\n",
      "lon    lat   month_year                                                     \n",
      "-179.5 -76.5 2015-01-01  5.759335  0.045419  9.845816  9.974246  0.027799   \n",
      "             2015-02-01  6.431987  0.060305  9.850572  9.974982  0.014049   \n",
      "             2015-12-01  8.597108  0.051439  9.861738  9.977141  0.022660   \n",
      "             2016-01-01  7.135804  0.035739  9.861491  9.978006  0.030432   \n",
      "             2016-02-01  8.560923  0.070589  9.837498  9.972045  0.016303   \n",
      "\n",
      "                                V  \n",
      "lon    lat   month_year            \n",
      "-179.5 -76.5 2015-01-01  0.045419  \n",
      "             2015-02-01  0.060305  \n",
      "             2015-12-01  0.051439  \n",
      "             2016-01-01  0.035739  \n",
      "             2016-02-01  0.070589  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1534438 entries, (np.float64(-179.5), np.float64(-76.5), Timestamp('2015-01-01 00:00:00')) to (np.float64(179.5), np.float64(77.5), Timestamp('2020-09-01 00:00:00'))\n",
      "Data columns (total 22 columns):\n",
      " #   Column   Non-Null Count    Dtype  \n",
      "---  ------   --------------    -----  \n",
      " 0   dfe      1534438 non-null  float64\n",
      " 1   no3      1534438 non-null  float64\n",
      " 2   po4      1534438 non-null  float64\n",
      " 3   sil      1534438 non-null  float64\n",
      " 4   rsn      1534438 non-null  float64\n",
      " 5   mld      1534438 non-null  float64\n",
      " 6   ssh      1534438 non-null  float64\n",
      " 7   sal      1534438 non-null  float64\n",
      " 8   sst      1534438 non-null  float64\n",
      " 9   chla     1534438 non-null  float64\n",
      " 10  logchla  1534438 non-null  float64\n",
      " 11  mcro     1534438 non-null  float64\n",
      " 12  nano     1534438 non-null  float64\n",
      " 13  pico     1534438 non-null  float64\n",
      " 14  logmcro  1534438 non-null  float64\n",
      " 15  lognano  1534438 non-null  float64\n",
      " 16  logpico  1534438 non-null  float64\n",
      " 17  V_dfe    1534438 non-null  float64\n",
      " 18  V_no3    1534438 non-null  float64\n",
      " 19  V_po4    1534438 non-null  float64\n",
      " 20  Tf       1534438 non-null  float64\n",
      " 21  V        1534438 non-null  float64\n",
      "dtypes: float64(22)\n",
      "memory usage: 264.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mu_ref = 5.0\n",
    "k_dfe = 3 ** (-0.5)\n",
    "k_no3 = 0.5\n",
    "k_po4 = 0.01\n",
    "ds['V_dfe'] = ds['dfe'] / (k_dfe + ds['dfe'])\n",
    "ds['V_no3'] = ds['no3'] / (k_no3 + ds['no3'])\n",
    "ds['V_po4'] = ds['po4'] / (k_po4 + ds['po4'])\n",
    "ds['Tf'] = 1.7 ** np.exp((ds['sst'] - 30) / 10)\n",
    "\n",
    "# Convert to NumPy arrays, compute the minimum, and then convert back to xarray\n",
    "V_no3_array = ds['V_no3'].values  # Convert to NumPy array\n",
    "V_po4_array = ds['V_po4'].values  # Convert to NumPy array\n",
    "V_dfe_array = ds['V_dfe'].values   # Assuming V_dfe is already defined\n",
    "\n",
    "# Calculate the minimum using NumPy\n",
    "V_min = np.minimum(np.minimum(V_dfe_array, V_po4_array), V_no3_array)\n",
    "\n",
    "# Assign the result back to the DataArray in the xarray Dataset\n",
    "ds['V'] = xr.DataArray(V_min, dims=ds['V_dfe'].dims, coords=ds['V_dfe'].coords)\n",
    "\n",
    "df = ds.to_dataframe()\n",
    "df = df.dropna(how='any')\n",
    "scaler = MinMaxScaler(feature_range=(0,10))\n",
    "scaled_array = scaler.fit_transform(df)\n",
    "norm_df = pd.DataFrame(scaled_array, index=df.index, columns=df.columns)\n",
    "print(norm_df.head())\n",
    "print(norm_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3373f623-b719-429d-b9ce-6029b891b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66744, 26)\n",
      "(1000, 26)\n"
     ]
    }
   ],
   "source": [
    "# Resetting the index to work with columns directly\n",
    "df = norm_df.sort_index().reset_index()\n",
    "df['month_year'] = pd.to_datetime(df['month_year'])\n",
    "start_date = df['month_year'].min()\n",
    "# df['months_from_start'] = (df['month_year'] - df['month_year'].min()) // pd.Timedelta('30D')\n",
    "df['months_from_start'] = ((df['month_year'].dt.year - start_date.year) * 12 + \n",
    "                                  (df['month_year'].dt.month - start_date.month))\n",
    "# Count months per (lon, lat)\n",
    "month_counts = df.groupby(['lon', 'lat'])['months_from_start'].nunique()\n",
    "expected_months = df['months_from_start'].nunique()\n",
    "valid_locations = month_counts[month_counts == expected_months].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_filtered = df.set_index(['lon', 'lat']).loc[valid_locations].reset_index()\n",
    "df_filtered = df_filtered.sort_values(['lon', 'lat', 'months_from_start'])\n",
    "print(df_filtered.shape)\n",
    "df_sampled = df_filtered.sample(n=1000, random_state=42).sort_values(['lon', 'lat', 'months_from_start'])\n",
    "print(df_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932c45a1-a7f4-4ae4-b9f5-cd5d231a72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and target columns\n",
    "input_cols = ['rsn', 'Tf', 'V', 'logchla']\n",
    "target_col = ['logmcro']\n",
    "\n",
    "# Group by location\n",
    "grouped = df_sampled.groupby(['lon', 'lat'])\n",
    "\n",
    "X_list, Y_list = [], []\n",
    "for _, group in grouped:\n",
    "    group = group.sort_values('months_from_start')\n",
    "    X = group[input_cols].values\n",
    "    Y = group[target_col].values\n",
    "    X_list.append(torch.tensor(X, dtype=torch.float32))\n",
    "    Y_list.append(torch.tensor(Y, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df45122-aea1-4fa9-8394-9d01a87ff715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed only if the sequences are of different lengths\n",
    "nsteps = max(len(x) for x in X_list)\n",
    "X_padded = torch.stack([torch.nn.functional.pad(x, (0, 0, 0, nsteps - x.shape[0])) for x in X_list])\n",
    "Y_padded = torch.stack([torch.nn.functional.pad(y, (0, 0, 0, nsteps - y.shape[0])) for y in Y_list])\n",
    "\n",
    "X_padded = X_padded.view(len(X_list), nsteps, -1)  # shape: (nbatch, nsteps, n_features)\n",
    "Y_padded = Y_padded.view(len(Y_list), nsteps, -1)  # shape: (nbatch, nsteps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3cfe68-54a7-4a71-8ee8-0f68c087931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Compute split indices\n",
    "nbatch = len(X_list)\n",
    "n_train = int(train_ratio * nbatch)\n",
    "n_val = int(val_ratio * nbatch)\n",
    "n_test = nbatch - n_train - n_val\n",
    "\n",
    "# Shuffle indices for reproducibility\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(nbatch)\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "val_idx = indices[n_train:n_train + n_val]\n",
    "test_idx = indices[n_train + n_val:]\n",
    "\n",
    "X_train = X_padded[train_idx]\n",
    "Y_train = Y_padded[train_idx]\n",
    "\n",
    "X_val = X_padded[val_idx]\n",
    "Y_val = Y_padded[val_idx]\n",
    "\n",
    "X_test = X_padded[test_idx]\n",
    "Y_test = Y_padded[test_idx]\n",
    "\n",
    "train_data = DictDataset({\n",
    "    'Y0':Y_train[:, 0, :],   # initial Pi (shape: [B, 1])\n",
    "    'X': X_train,            # external inputs (shape: [B, T, 4])\n",
    "    'Y': Y_train             # ground truth Pi sequence\n",
    "}, name='train')\n",
    "\n",
    "val_data = DictDataset({\n",
    "    'Y0': Y_val[:, 0, :],\n",
    "    'X': X_val,\n",
    "    'Y': Y_val\n",
    "}, name='val')\n",
    "\n",
    "test_data = DictDataset({\n",
    "    'Y0': Y_test[:, 0, :],\n",
    "    'X': X_test,\n",
    "    'Y': Y_test\n",
    "}, name='test')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, collate_fn=train_data.collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, collate_fn=val_data.collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, collate_fn=test_data.collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060ca21a-6439-4e25-a501-f58a0809d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromancer import blocks\n",
    "\n",
    "# define UDE system in Neuromancer\n",
    "class PlanktonHybrid(ode.ODESystem):\n",
    "    def __init__(self, block, insize=5, outsize=1):\n",
    "        \"\"\"\n",
    "        :param block: Neural block (e.g. MLP) that learns unknown dynamics\n",
    "        :param insize: Number of input features (rsn, Tf, V, logchla)\n",
    "        :param outsize: Number of output features (logmcro)\n",
    "        \"\"\"\n",
    "        super().__init__(insize=insize, outsize=outsize)\n",
    "        self.block = block\n",
    "        \n",
    "        # Mechanistic parameters\n",
    "        self.mu_ref = torch.tensor(5.0)\n",
    "        self.alpha_chl =torch.tensor(0.28)\n",
    "        self.m_i = nn.Parameter(torch.tensor(0.05), requires_grad=True)\n",
    "        self.alpha_i = nn.Parameter(torch.tensor(0.01), requires_grad=True)\n",
    "        assert self.block.in_features == 5\n",
    "        assert self.block.out_features == 2\n",
    "        \n",
    "    def ode_equations(self, Pi, U):\n",
    "        \"\"\"\n",
    "        Pi: shape (B, 1) — current plankton biomass\n",
    "        U: shape (B, 4) — external inputs [I, Tf, V, logchla]\n",
    "        \"\"\"\n",
    "        # Concatenate inputs and state\n",
    "        xu = torch.cat([U, Pi], dim=-1)  # shape: (B, 5)\n",
    "\n",
    "        # Neural block outputs\n",
    "        learned_theta, learned_graz = torch.split(self.block(xu), 1, dim=-1)\n",
    "        # ensure positive grazing via softplus (or relu)\n",
    "        learned_graz = F.softplus(learned_graz_raw)\n",
    "\n",
    "        # Extract environmental drivers\n",
    "        I  = U[:, [0]]\n",
    "        Tf = U[:, [1]]\n",
    "        V  = U[:, [2]]\n",
    "        logchla = U[:, [3]]    \n",
    "\n",
    "        # Mechanistic dynamics\n",
    "        denom = torch.clamp(self.mu_ref * Tf * V, min=1e-6)\n",
    "        exponent = torch.clamp(-self.alpha_chl * learned_theta * I / denom, min=-50.0, max=50.0)\n",
    "        growth = self.mu_ref * Tf * (1 - torch.exp(exponent)) * V * Pi\n",
    "        mortality = self.m_i * Tf * Pi\n",
    "        aggregation = self.alpha_i * (Pi ** 1.75)\n",
    "\n",
    "        # Final dynamics\n",
    "        dPi = growth - learned_graz - mortality - aggregation\n",
    "        \n",
    "        # final safety clamp\n",
    "        dPi = torch.nan_to_num(dPi, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        return dPi\n",
    "\n",
    "# Define a neural block with 5 inputs and 2 outputs\n",
    "neural_block = blocks.MLP(\n",
    "    insize=5,\n",
    "    outsize=2,\n",
    "    hsizes=[64, 64],             # two hidden layers\n",
    "    nonlin=torch.nn.ReLU,        # activation function\n",
    "    bias=True,\n",
    "    linear_map=torch.nn.Linear\n",
    ")\n",
    "\n",
    "# Non-autonomous\n",
    "class PlanktonNode(Node):\n",
    "    def forward(self, Y0, X):\n",
    "        \"\"\"\n",
    "        Y0: initial state, shape (B, state_dim)\n",
    "        X: sequence of inputs, shape (B, T, input_dim)\n",
    "        returns: Y_pred of shape (B, T, state_dim)\n",
    "        \"\"\"\n",
    "        B, T, _ = X.shape\n",
    "        Pi = Y0\n",
    "        Y_pred_seq = []\n",
    "\n",
    "        for t in range(T):\n",
    "            U_t = X[:, t, :]       # shape (B, input_dim)\n",
    "            Pi = fxRK4(Pi, U_t)   # RK4 step: Pi (B, state_dim), U_t (B, input_dim)\n",
    "            Y_pred_seq.append(Pi)\n",
    "\n",
    "        Y_pred = torch.stack(Y_pred_seq, dim=1)  # (B, T, state_dim)\n",
    "        return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e41bb1-da1d-44d5-9440-0225a5622907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your hybrid system\n",
    "fx = PlanktonHybrid(block=neural_block)\n",
    "\n",
    "ts = 1.0  # time step size (e.g. 1 month)\n",
    "# integrate UDE model\n",
    "fxRK4 = integrators.RK4(fx, h=ts)\n",
    "\n",
    "# create symbolic UDE model\n",
    "plankton_node = Node(\n",
    "    callable=fxRK4,\n",
    "    input_keys=['Y0', 'X'],      # Y0 = initial Pi, X = external inputs\n",
    "    output_keys=['Y_pred'],     # predicted output sequence\n",
    "    name='PlanktonUDE'\n",
    ")\n",
    "dynamics_model = System(\n",
    "    nodes=[plankton_node],\n",
    "    nsteps=nsteps             # e.g. 72 months\n",
    ")\n",
    "\n",
    "# define symbolic losses\n",
    "Y = variable('Y')             # true sequence\n",
    "Y_pred = variable('Y_pred')   # predicted sequence\n",
    "\n",
    "# trajectory tracking loss\n",
    "tracking_loss = 1.0 * ((Y_pred == Y)^2)\n",
    "tracking_loss.name = 'tracking_loss'\n",
    "\n",
    "# One-step loss\n",
    "onestep_loss = 1.0 * ((Y_pred[:, 1, :] == Y[:, 1, :])^2)\n",
    "onestep_loss.name = 'onestep_loss'\n",
    "\n",
    "# Finite difference loss\n",
    "Y_fd = Y[:, 1:, :] - Y[:, :-1, :]\n",
    "Y_pred_fd = Y_pred[:, 1:, :] - Y_pred[:, :-1, :]\n",
    "fd_loss = 2.0 * ((Y_fd == Y_pred_fd)^2)\n",
    "fd_loss.name = 'FD_loss'\n",
    "\n",
    "# Initial condition consistency\n",
    "init_loss = (Y_pred[:, 0, :] == Y[:, 0, :])^2\n",
    "init_loss.name = 'init_loss'\n",
    "\n",
    "# # parameter regularisation loss\n",
    "# alpha_i = variable('PlanktonUDE.alpha_i')\n",
    "# m_i = variable('PlanktonUDE.m_i')\n",
    "# reg_loss = 0.01 * (alpha_i ** 2 + m_i ** 2)\n",
    "# reg_loss.name = 'symbolic_reg'\n",
    "\n",
    "# add weights\n",
    "tracking_loss.weight = 1.0\n",
    "onestep_loss.weight = 1.0\n",
    "fd_loss.weight = 2.0\n",
    "init_loss.weight = 0.5\n",
    "# reg_loss.weight = 0.01\n",
    "\n",
    "# aggregate list of objective terms and constraints\n",
    "objectives = [tracking_loss, onestep_loss, fd_loss, init_loss]\n",
    "constraints = []\n",
    "\n",
    "# create constrained optimization loss\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem(\n",
    "    [dynamics_model], \n",
    "    loss,\n",
    "    grad_inference=True     # enables symbolic gradient flow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e862062-bcf6-4611-832a-f50c56191d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() elif device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)\n",
    "problem.to(device)\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=0.003)\n",
    "\n",
    "trainer = Trainer(\n",
    "    problem,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_data,\n",
    "    optimizer,\n",
    "    patience=5,\n",
    "    warmup=5,\n",
    "    epochs=10,\n",
    "    eval_metric=\"val_loss\",\n",
    "    train_metric=\"train_loss\",\n",
    "    dev_metric=\"val_loss\",\n",
    "    test_metric=\"val_loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ac13269-4d0a-498c-8751-bcc806e58f4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m problem\u001b[38;5;241m.\u001b[39mload_state_dict(best_model)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\trainer.py:246\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    244\u001b[0m t_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m    245\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m move_batch_to_device(t_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 246\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_fidelity:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnodes:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\problem.py:198\u001b[0m, in \u001b[0;36mProblem.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 198\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(output_dict)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\problem.py:206\u001b[0m, in \u001b[0;36mProblem.step\u001b[1;34m(self, input_dict)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dict: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[1;32m--> 206\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    208\u001b[0m             output_dict \u001b[38;5;241m=\u001b[39m {node\u001b[38;5;241m.\u001b[39mname: output_dict}\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\system.py:275\u001b[0m, in \u001b[0;36mSystem.forward\u001b[1;34m(self, input_dict)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m    274\u001b[0m         indata \u001b[38;5;241m=\u001b[39m {k: data[k][:, i] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minput_keys}  \u001b[38;5;66;03m# collect what the compute node needs from data nodes\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m         outdata \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# compute\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat(data, outdata)  \u001b[38;5;66;03m# feed the data nodes\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\system.py:56\u001b[0m, in \u001b[0;36mNode.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mThis call function wraps the callable to receive/send dictionaries of Tensors\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m:param datadict: (dict {str: Tensor}) input to callable with associated input_keys\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m:return: (dict {str: Tensor}) Output of callable with associated output_keys\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [data[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys]\n\u001b[1;32m---> 56\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     58\u001b[0m     output \u001b[38;5;241m=\u001b[39m [output]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\dynamics\\integrators.py:40\u001b[0m, in \u001b[0;36mIntegrator.forward\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    This function needs x only for autonomous systems. x is 2D.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    It needs x and u for nonautonomous system w/ online interpolation. x and u are 2D tensors.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\dynamics\\integrators.py:207\u001b[0m, in \u001b[0;36mRK4.integrate\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mintegrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    206\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh\n\u001b[1;32m--> 207\u001b[0m     k1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m                    \u001b[38;5;66;03m# k1 = f(x_i, t_i)\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     k2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(x \u001b[38;5;241m+\u001b[39m h\u001b[38;5;241m*\u001b[39mk1\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m*\u001b[39margs)         \u001b[38;5;66;03m# k2 = f(x_i + 0.5*h*k1, t_i + 0.5*h)\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     k3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(x \u001b[38;5;241m+\u001b[39m h\u001b[38;5;241m*\u001b[39mk2\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m*\u001b[39margs)         \u001b[38;5;66;03m# k3 = f(x_i + 0.5*h*k2, t_i + 0.5*h)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\neuromancer\\lib\\site-packages\\neuromancer\\dynamics\\ode.py:71\u001b[0m, in \u001b[0;36mODESystem.forward\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mode_equations(x, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = trainer.train()\n",
    "problem.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358eb7e4-9a89-47a5-b35c-2c45b2e85625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
